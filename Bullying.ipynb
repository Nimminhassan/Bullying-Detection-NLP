{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP_9hQtxAJsk"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GKv2hr-7BH_"
      },
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "!pip install emoji scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import emoji\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0MP1FVS7Qv_"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert emojis ‚Üí text labels (e.g., üò° -> :angry_face:)\n",
        "    text = emoji.demojize(text, language='en')\n",
        "    # Replace underscores with spaces to improve TF-IDF tokenization\n",
        "    text = text.replace(\"_\", \" \")\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qvhr4c88wKO"
      },
      "outputs": [],
      "source": [
        "!pip install pandas odfpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x56av7ba7VLO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"Dataset.ods\")\n",
        "\n",
        "df = df.rename(columns={\"COMMENTS\": \"text\", \"LABELS\": \"label\"})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u8o2XF78qJw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess emojis\n",
        "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzgTp1fx9AFX"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpqGA7V09DkX"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dClWMdVX9e4I"
      },
      "outputs": [],
      "source": [
        "svm_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        sublinear_tf=True,\n",
        "        max_features=2000,\n",
        "        ngram_range=(1, 2)   # unigrams + bigrams improve bullying detection\n",
        "    )),\n",
        "    ('svm', LinearSVC(class_weight=\"balanced\"))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DvczNLJ9k3Q"
      },
      "outputs": [],
      "source": [
        "svm_clf.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO0KILd-9tJQ"
      },
      "outputs": [],
      "source": [
        "svm_preds = svm_clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, svm_preds))\n",
        "print(classification_report(y_test, svm_preds, target_names=le.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1k35mW29wII"
      },
      "outputs": [],
      "source": [
        "def predict_comment(text):\n",
        "    text = preprocess_text(text)\n",
        "    pred = svm_clf.predict([text])[0]\n",
        "    return le.inverse_transform([pred])[0]\n",
        "\n",
        "print(predict_comment(\"You are so stupid üò°\"))\n",
        "print(predict_comment(\"Have a nice day üòä\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J358uRJeAQ47"
      },
      "source": [
        "**RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_jxpzjGAc8d"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYsJGpIlA2AW"
      },
      "outputs": [],
      "source": [
        "# 1. Optional: Preprocess emojis ‚Üí readable words\n",
        "# -------------------------------------------------------\n",
        "def preprocess_text(text):\n",
        "    # convert üò° ‚Üí :angry_face:\n",
        "    text = emoji.demojize(text, language=\"en\")\n",
        "    return text.replace(\"_\", \" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8J5G4WKA5Cl"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChIrrcgSBJQu"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(\n",
        "    max_features=20000,\n",
        "    ngram_range=(1, 2),   # unigrams + bigrams\n",
        "    sublinear_tf=True\n",
        ")\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjz27IqFBuIm"
      },
      "outputs": [],
      "source": [
        "# 4. Random Forest model\n",
        "# -------------------------------------------------------\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400,         # number of trees\n",
        "    max_depth=None,           # grow trees fully\n",
        "    class_weight=\"balanced\",  # handle bullying imbalance\n",
        "    n_jobs=-1,                # use all CPU cores\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKqytGXuBzFm"
      },
      "outputs": [],
      "source": [
        "# 5. Predict & evaluate\n",
        "# -------------------------------------------------------\n",
        "rafo_preds = rf.predict(X_test_tfidf)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rafo_preds))\n",
        "print(classification_report(y_test, rafo_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPOmctmLB7xW"
      },
      "outputs": [],
      "source": [
        "def predict_comment1(text):\n",
        "    text = preprocess_text(text)\n",
        "    pred = rf.predict([text])[0]\n",
        "    return le.inverse_transform([pred])[0]\n",
        "\n",
        "print(predict_comment(\"You are so stupid üò°\"))\n",
        "print(predict_comment(\"Have a nice day üòä\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmAX3HuwDKD-"
      },
      "source": [
        "**DISTILBERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seTefHjSDJuu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets emoji scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crQo41MHDQJ3"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example structure: df[\"text\"], df[\"label\"]\n",
        "# label: 1 = Non-bullying, 0 = Bullying\n",
        "\n",
        "def clean_text(text):\n",
        "    text = emoji.demojize(text, language=\"en\")  # üò° -> :angry_face:\n",
        "    return text.replace(\"_\", \" \")\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label\"],\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq0PB_2eD29w"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"clean_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs-B5HI7D6h3"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "test_ds = test_ds.map(tokenize, batched=True)\n",
        "\n",
        "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
        "test_ds = test_ds.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_ds.set_format(type=\"torch\")\n",
        "test_ds.set_format(type=\"torch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbSBFTCGEOPY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"macro\")\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d__uxhXERxY"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aEgH6-tETu4"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=20,\n",
        "\n",
        "    fp16=False,         # True if GPU supports FP16\n",
        "    report_to=\"none\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghddv6aOEZOI"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZPb2mMMEbVg"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR4sTsZ8H6Pa"
      },
      "outputs": [],
      "source": [
        "model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRhbpaQ0HppS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def bert_predict(text):\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # 1. Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # 2. Move inputs to the same device as model\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # 3. Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    bert_preds = outputs.logits.argmax(dim=1).item()\n",
        "    return \"Bullying\" if bert_preds == 0 else \"Not bullying\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtZDGCJmGvqS"
      },
      "outputs": [],
      "source": [
        "print(bert_predict(\"Red\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFwgoE6dIQWr"
      },
      "outputs": [],
      "source": [
        "bert_preds = []\n",
        "for text in X_test:\n",
        "    bert_preds.append(0 if bert_predict(text) == \"Bullying\" else 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEp-ptMCJrQL"
      },
      "source": [
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC3b2NwEJvo0"
      },
      "outputs": [],
      "source": [
        "pip install emoji torch numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLCDvbUgJyQ8"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = emoji.demojize(text, language=\"en\")  # üò° ‚Üí :angry_face:\n",
        "    text = text.replace(\"_\", \" \")\n",
        "    text = re.sub(r\":[a-zA-Z0-9 ]+:\", \"\", text)  # remove :emoji names:\n",
        "    return text.lower().strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ApdellIJzpM"
      },
      "outputs": [],
      "source": [
        "X = df[\"text\"].tolist()\n",
        "y = df[\"label\"].tolist()   # 1 = non-bullying, 0 = bullying\n",
        "\n",
        "X = [clean_text(t) for t in X]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3qCpX0HJ63l"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(texts, vocab_size=20000):\n",
        "    words = \" \".join(texts).split()\n",
        "    freq = Counter(words)\n",
        "    most_common = freq.most_common(vocab_size - 2)\n",
        "\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    vocab.update({word: i + 2 for i, (word, _) in enumerate(most_common)})\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(X_train, vocab_size=20000)\n",
        "\n",
        "def encode(text, max_len=40):\n",
        "    ids = [vocab.get(w, 1) for w in text.split()]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [0] * (max_len - len(ids))\n",
        "    return ids[:max_len]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LePsevq9K-ts"
      },
      "outputs": [],
      "source": [
        "max_len = 40\n",
        "\n",
        "X_train_ids = [encode(t, max_len) for t in X_train]\n",
        "X_test_ids  = [encode(t, max_len) for t in X_test]\n",
        "\n",
        "X_train_ids = torch.tensor(X_train_ids)\n",
        "X_test_ids  = torch.tensor(X_test_ids)\n",
        "y_train     = torch.tensor(y_train)\n",
        "y_test      = torch.tensor(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll80fRfSK_d-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    TensorDataset(X_train_ids, y_train),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dl = DataLoader(\n",
        "    TensorDataset(X_test_ids, y_test),\n",
        "    batch_size=batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmzWVy3nLJ4V"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=256, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (h, c) = self.lstm(x)\n",
        "        out = self.fc(h[-1])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keFHX0SOLLY9"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = LSTMClassifier(vocab_size=len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for Xb, yb in train_dl:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(Xb)\n",
        "\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KFm8_38LNCV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_dl:\n",
        "        Xb = Xb.to(device)\n",
        "        preds = model(Xb).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(yb.numpy())\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "print(\"F1 Score:\", f1_score(all_labels, all_preds, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# recreate tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n"
      ],
      "metadata": {
        "id": "SVpaiNsfg2k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_predict(text):\n",
        "    # Use the same tokenizer used during LSTM training (Keras tokenizer)\n",
        "    text = clean_text(text)\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([text])      # ‚ùó must be keras tokenizer\n",
        "    padded = pad_sequences(seq, maxlen=max_len)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    X = torch.tensor(padded).long().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(X)\n",
        "        pred = output.argmax(dim=1).item()\n",
        "\n",
        "    return \"Bullying\" if pred == 0 else \"Not bullying\"\n"
      ],
      "metadata": {
        "id": "LLF8AbkFfqc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_predict(\"Saale randi\")"
      ],
      "metadata": {
        "id": "DTfYBKiq-xjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKrZ6MjHZX0W"
      },
      "outputs": [],
      "source": [
        "lstm_preds = []\n",
        "for text in X_test:\n",
        "    lstm_preds.append(0 if lstm_predict(text) == \"Bullying\" else 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQquOUCmPmbx"
      },
      "source": [
        "**LINEAR REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srVkFe1iR3B4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cQx3hozR-NI"
      },
      "outputs": [],
      "source": [
        "log_reg_clf = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        max_features=3000,          # increase for better accuracy\n",
        "        ngram_range=(1,2),          # unigrams + bigrams\n",
        "        preprocessor=preprocess_text\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight=\"balanced\",    # handles imbalanced bullying data\n",
        "        max_iter=300                # increase iterations for convergence\n",
        "    ))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luB0kzOxSIV4"
      },
      "outputs": [],
      "source": [
        "log_reg_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_i-9IR4SSSi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "log_preds = log_reg_clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, log_preds))\n",
        "print(classification_report(y_test, log_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmSNb3scSZIR"
      },
      "outputs": [],
      "source": [
        "def predict_comment(text):\n",
        "    pred = log_reg_clf.predict([text])[0]\n",
        "    return \"Bullying\" if pred == 0 else \"Not bullying\"\n",
        "print(predict_comment(\"Red\"))\n",
        "print(predict_comment(\"Have a nice day üòä\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvSggOiVSeuJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BRG6qhjWagc"
      },
      "source": [
        "**COMPARISON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaYIi19xWf6E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_model(name, true, pred, results):\n",
        "    acc = accuracy_score(true, pred)\n",
        "    prec = precision_score(true, pred, average=\"binary\")\n",
        "    rec = recall_score(true, pred, average=\"binary\")\n",
        "    f1 = f1_score(true, pred, average=\"binary\")\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1 Score\": f1\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdiegAMtWjCb"
      },
      "outputs": [],
      "source": [
        "results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK1IR2i5Wi_j"
      },
      "outputs": [],
      "source": [
        "evaluate_model(\"Logistic Regression\", y_test, log_preds, results)\n",
        "evaluate_model(\"SVM\", y_test, svm_preds, results)\n",
        "evaluate_model(\"Random Forest\", y_test, rafo_preds, results)\n",
        "evaluate_model(\"DistilBERT\", y_test, bert_preds, results)\n",
        "evaluate_model(\"LSTM\", y_test, lstm_preds, results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQmdkNJRZGqF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPVgqt2kZHb3"
      },
      "outputs": [],
      "source": [
        "df_results.plot(x=\"Model\", y=[\"Accuracy\", \"F1 Score\"], kind=\"bar\", figsize=(10,5))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "tqIybVX_8GWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"LSTM\": lstm_preds,\n",
        "    \"DistilBERT\": bert_preds,\n",
        "    \"Random Forest\": rafo_preds,\n",
        "    \"SVM\": svm_preds,\n",
        "    \"Logistic Regression\": log_preds\n",
        "}\n",
        "\n",
        "for name, pred in models.items():\n",
        "    cm = confusion_matrix(y_test, pred)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Not Bullying\", \"Bullying\"])\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "    plt.title(f\"{name} - Confusion Matrix\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qMvQsXmOuFgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbformat nbconvert\n",
        "!jupyter nbconvert --to notebook --ClearOutputPreprocessor.enabled=True Bullying.ipynb\n"
      ],
      "metadata": {
        "id": "3tHV7iljuFWu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}